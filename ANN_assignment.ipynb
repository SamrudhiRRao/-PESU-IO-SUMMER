{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = keras.datasets.imdb\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training entries: 25000, labels: 25000\n"
     ]
    }
   ],
   "source": [
    "print(\"Training entries: {}, labels: {}\".format(len(train_data), len(train_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(218, 189)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data[0]), len(train_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "\n",
    "# The first indices are reserved\n",
    "word_index = {k:(v+3) for k,v in word_index.items()}\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2  # unknown\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<START> french horror cinema has seen something of a revival over the last couple of years with great films such as inside and <UNK> romance <UNK> on to the scene <UNK> <UNK> the revival just slightly but stands head and shoulders over most modern horror titles and is surely one of the best french horror films ever made <UNK> was obviously shot on a low budget but this is made up for in far more ways than one by the originality of the film and this in turn is <UNK> by the excellent writing and acting that ensure the film is a winner the plot focuses on two main ideas prison and black magic the central character is a man named <UNK> sent to prison for fraud he is put in a cell with three others the quietly insane <UNK> body building <UNK> marcus and his retarded boyfriend daisy after a short while in the cell together they stumble upon a hiding place in the wall that contains an old <UNK> after <UNK> part of it they soon realise its magical powers and realise they may be able to use it to break through the prison walls br br black magic is a very interesting topic and i'm actually quite surprised that there aren't more films based on it as there's so much scope for things to do with it it's fair to say that <UNK> makes the best of it's <UNK> as despite it's <UNK> the film never actually feels restrained and manages to flow well throughout director eric <UNK> provides a great atmosphere for the film the fact that most of it takes place inside the central prison cell <UNK> that the film feels very claustrophobic and this immensely benefits the central idea of the prisoners wanting to use magic to break out of the cell it's very easy to get behind them it's often said that the unknown is the thing that really <UNK> people and this film proves that as the director <UNK> that we can never really be sure of exactly what is round the corner and this helps to ensure that <UNK> actually does manage to be quite frightening the film is memorable for a lot of reasons outside the central plot the characters are all very interesting in their own way and the fact that the book itself almost takes on its own character is very well done anyone worried that the film won't deliver by the end won't be disappointed either as the ending both makes sense and manages to be quite horrifying overall <UNK> is a truly great horror film and one of the best of the decade highly recommended viewing\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_review(train_data[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = keras.preprocessing.sequence.pad_sequences(train_data,\n",
    "                                                        value=word_index[\"<PAD>\"],\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=256)\n",
    "\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(test_data,\n",
    "                                                       value=word_index[\"<PAD>\"],\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 256)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data[0]), len(train_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1   14   22   16   43  530  973 1622 1385   65  458 4468   66 3941\n",
      "    4  173   36  256    5   25  100   43  838  112   50  670    2    9\n",
      "   35  480  284    5  150    4  172  112  167    2  336  385   39    4\n",
      "  172 4536 1111   17  546   38   13  447    4  192   50   16    6  147\n",
      " 2025   19   14   22    4 1920 4613  469    4   22   71   87   12   16\n",
      "   43  530   38   76   15   13 1247    4   22   17  515   17   12   16\n",
      "  626   18    2    5   62  386   12    8  316    8  106    5    4 2223\n",
      " 5244   16  480   66 3785   33    4  130   12   16   38  619    5   25\n",
      "  124   51   36  135   48   25 1415   33    6   22   12  215   28   77\n",
      "   52    5   14  407   16   82    2    8    4  107  117 5952   15  256\n",
      "    4    2    7 3766    5  723   36   71   43  530  476   26  400  317\n",
      "   46    7    4    2 1029   13  104   88    4  381   15  297   98   32\n",
      " 2071   56   26  141    6  194 7486   18    4  226   22   21  134  476\n",
      "   26  480    5  144   30 5535   18   51   36   28  224   92   25  104\n",
      "    4  226   65   16   38 1334   88   12   16  283    5   16 4472  113\n",
      "  103   32   15   16 5345   19  178   32    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0707 22:35:29.470194 13308 deprecation.py:506] From C:\\Users\\Ramesh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0707 22:35:29.497086 13308 deprecation.py:506] From C:\\Users\\Ramesh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 160,289\n",
      "Trainable params: 160,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# input shape is the vocabulary count used for the movie reviews (10,000 words)\n",
    "vocab_size = 10000\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, 16))\n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "model.add(keras.layers.Dense(16, activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0707 22:35:29.592832 13308 deprecation.py:323] From C:\\Users\\Ramesh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = train_data[:10000]\n",
    "partial_x_train = train_data[10000:]\n",
    "\n",
    "y_val = train_labels[:10000]\n",
    "partial_y_train = train_labels[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      "15000/15000 [==============================] - 1s 80us/sample - loss: 0.6920 - acc: 0.6216 - val_loss: 0.6902 - val_acc: 0.6394\n",
      "Epoch 2/40\n",
      "15000/15000 [==============================] - 1s 70us/sample - loss: 0.6866 - acc: 0.6961 - val_loss: 0.6826 - val_acc: 0.7444\n",
      "Epoch 3/40\n",
      "15000/15000 [==============================] - 1s 79us/sample - loss: 0.6746 - acc: 0.7571 - val_loss: 0.6672 - val_acc: 0.7545\n",
      "Epoch 4/40\n",
      "15000/15000 [==============================] - 1s 73us/sample - loss: 0.6530 - acc: 0.7627 - val_loss: 0.6422 - val_acc: 0.7555\n",
      "Epoch 5/40\n",
      "15000/15000 [==============================] - 1s 78us/sample - loss: 0.6205 - acc: 0.7919 - val_loss: 0.6079 - val_acc: 0.7818\n",
      "Epoch 6/40\n",
      "15000/15000 [==============================] - 1s 88us/sample - loss: 0.5782 - acc: 0.8124 - val_loss: 0.5655 - val_acc: 0.8024\n",
      "Epoch 7/40\n",
      "15000/15000 [==============================] - 1s 84us/sample - loss: 0.5278 - acc: 0.8275 - val_loss: 0.5173 - val_acc: 0.8191\n",
      "Epoch 8/40\n",
      "15000/15000 [==============================] - 1s 71us/sample - loss: 0.4757 - acc: 0.8464 - val_loss: 0.4722 - val_acc: 0.8342\n",
      "Epoch 9/40\n",
      "15000/15000 [==============================] - 1s 72us/sample - loss: 0.4277 - acc: 0.8627 - val_loss: 0.4325 - val_acc: 0.8455\n",
      "Epoch 10/40\n",
      "15000/15000 [==============================] - 1s 93us/sample - loss: 0.3863 - acc: 0.8751 - val_loss: 0.4007 - val_acc: 0.8531\n",
      "Epoch 11/40\n",
      "15000/15000 [==============================] - 1s 91us/sample - loss: 0.3522 - acc: 0.8832 - val_loss: 0.3754 - val_acc: 0.8610\n",
      "Epoch 12/40\n",
      "15000/15000 [==============================] - 1s 91us/sample - loss: 0.3241 - acc: 0.8909 - val_loss: 0.3563 - val_acc: 0.8654\n",
      "Epoch 13/40\n",
      "15000/15000 [==============================] - 1s 81us/sample - loss: 0.3014 - acc: 0.8977 - val_loss: 0.3401 - val_acc: 0.8710\n",
      "Epoch 14/40\n",
      "15000/15000 [==============================] - 1s 87us/sample - loss: 0.2813 - acc: 0.9032 - val_loss: 0.3284 - val_acc: 0.8740\n",
      "Epoch 15/40\n",
      "15000/15000 [==============================] - 1s 85us/sample - loss: 0.2645 - acc: 0.9085 - val_loss: 0.3190 - val_acc: 0.8755\n",
      "Epoch 16/40\n",
      "15000/15000 [==============================] - 1s 86us/sample - loss: 0.2495 - acc: 0.9133 - val_loss: 0.3112 - val_acc: 0.8753\n",
      "Epoch 17/40\n",
      "15000/15000 [==============================] - 1s 78us/sample - loss: 0.2356 - acc: 0.9179 - val_loss: 0.3048 - val_acc: 0.8795\n",
      "Epoch 18/40\n",
      "15000/15000 [==============================] - 1s 83us/sample - loss: 0.2234 - acc: 0.9229 - val_loss: 0.2997 - val_acc: 0.8817\n",
      "Epoch 19/40\n",
      "15000/15000 [==============================] - 1s 73us/sample - loss: 0.2123 - acc: 0.9251 - val_loss: 0.2951 - val_acc: 0.8824\n",
      "Epoch 20/40\n",
      "15000/15000 [==============================] - 1s 73us/sample - loss: 0.2024 - acc: 0.9292 - val_loss: 0.2922 - val_acc: 0.8827\n",
      "Epoch 21/40\n",
      "15000/15000 [==============================] - 1s 79us/sample - loss: 0.1922 - acc: 0.9351 - val_loss: 0.2897 - val_acc: 0.8832\n",
      "Epoch 22/40\n",
      "15000/15000 [==============================] - 1s 72us/sample - loss: 0.1837 - acc: 0.9389 - val_loss: 0.2880 - val_acc: 0.8853\n",
      "Epoch 23/40\n",
      "15000/15000 [==============================] - 1s 72us/sample - loss: 0.1752 - acc: 0.9431 - val_loss: 0.2875 - val_acc: 0.8843\n",
      "Epoch 24/40\n",
      "15000/15000 [==============================] - 1s 73us/sample - loss: 0.1678 - acc: 0.9463 - val_loss: 0.2863 - val_acc: 0.8851\n",
      "Epoch 25/40\n",
      "15000/15000 [==============================] - 1s 71us/sample - loss: 0.1602 - acc: 0.9492 - val_loss: 0.2856 - val_acc: 0.8858\n",
      "Epoch 26/40\n",
      "15000/15000 [==============================] - 1s 93us/sample - loss: 0.1534 - acc: 0.9516 - val_loss: 0.2868 - val_acc: 0.8834\n",
      "Epoch 27/40\n",
      "15000/15000 [==============================] - 1s 83us/sample - loss: 0.1470 - acc: 0.9544 - val_loss: 0.2867 - val_acc: 0.8852\n",
      "Epoch 28/40\n",
      "15000/15000 [==============================] - 1s 99us/sample - loss: 0.1410 - acc: 0.9564 - val_loss: 0.2877 - val_acc: 0.8847\n",
      "Epoch 29/40\n",
      "15000/15000 [==============================] - 1s 83us/sample - loss: 0.1356 - acc: 0.9597 - val_loss: 0.2901 - val_acc: 0.8838\n",
      "Epoch 30/40\n",
      "15000/15000 [==============================] - 1s 80us/sample - loss: 0.1299 - acc: 0.9609 - val_loss: 0.2898 - val_acc: 0.8856\n",
      "Epoch 31/40\n",
      "15000/15000 [==============================] - 1s 89us/sample - loss: 0.1243 - acc: 0.9640 - val_loss: 0.2915 - val_acc: 0.8848\n",
      "Epoch 32/40\n",
      "15000/15000 [==============================] - 1s 92us/sample - loss: 0.1192 - acc: 0.9669 - val_loss: 0.2935 - val_acc: 0.8853\n",
      "Epoch 33/40\n",
      "15000/15000 [==============================] - 1s 73us/sample - loss: 0.1143 - acc: 0.9679 - val_loss: 0.2963 - val_acc: 0.8843\n",
      "Epoch 34/40\n",
      "15000/15000 [==============================] - 1s 75us/sample - loss: 0.1100 - acc: 0.9693 - val_loss: 0.2989 - val_acc: 0.8844\n",
      "Epoch 35/40\n",
      "15000/15000 [==============================] - 1s 75us/sample - loss: 0.1058 - acc: 0.9700 - val_loss: 0.3024 - val_acc: 0.8843\n",
      "Epoch 36/40\n",
      "15000/15000 [==============================] - 1s 77us/sample - loss: 0.1015 - acc: 0.9727 - val_loss: 0.3044 - val_acc: 0.8835\n",
      "Epoch 37/40\n",
      "15000/15000 [==============================] - 1s 79us/sample - loss: 0.0973 - acc: 0.9739 - val_loss: 0.3075 - val_acc: 0.8825\n",
      "Epoch 38/40\n",
      "15000/15000 [==============================] - 1s 82us/sample - loss: 0.0935 - acc: 0.9757 - val_loss: 0.3117 - val_acc: 0.8819\n",
      "Epoch 39/40\n",
      "15000/15000 [==============================] - 1s 83us/sample - loss: 0.0904 - acc: 0.9767 - val_loss: 0.3159 - val_acc: 0.8811\n",
      "Epoch 40/40\n",
      "15000/15000 [==============================] - 1s 95us/sample - loss: 0.0864 - acc: 0.9785 - val_loss: 0.3187 - val_acc: 0.8814\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=40,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 1s 34us/sample - loss: 0.3405 - acc: 0.8697\n",
      "[0.3405001765203476, 0.86972]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_data, test_labels)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
